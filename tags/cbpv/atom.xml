<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title> - cbpv</title>
    <link href="https://burakemir.ch/tags/cbpv/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://burakemir.ch"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2023-09-16T00:00:00+00:00</updated>
    <id>https://burakemir.ch/tags/cbpv/atom.xml</id>
    <entry xml:lang="en">
        <title>CBPV and Natural Deduction - Part 4. Polarized Logic</title>
        <published>2023-09-16T00:00:00+00:00</published>
        <updated>2023-09-16T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://burakemir.ch/post/cbpv-pt4-polarized/" type="text/html"/>
        <id>https://burakemir.ch/post/cbpv-pt4-polarized/</id>
        <content type="html">&lt;p&gt;This is the last and final part of my little study of polarized
natural deduction by means of CBPV. The previous parts are 
&lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt1-small-steps&#x2F;&quot;&gt;&amp;quot;pt1. small steps&amp;quot;&lt;&#x2F;a&gt;, 
&lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt2-sum-product&#x2F;&quot;&gt;&amp;quot;pt2. sums and products&amp;quot;&lt;&#x2F;a&gt;, and
&lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt3-linear-logic&#x2F;&quot;&gt;&amp;quot;pt3. linear logic&amp;quot;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In this part, I will sum up what I find this interesting. This is
going to be a bit more personal&#x2F;opinionated&#x2F;colored than the previous
parts.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;proof-theoretical-semantics&quot;&gt;Proof-theoretical semantics&lt;&#x2F;h2&gt;
&lt;p&gt;CBPV as polarized natural deduction helps develop an intuition for &amp;quot;proof-theoretical semantics.&amp;quot;&lt;&#x2F;p&gt;
&lt;p&gt;It may be a subjective preference, but I consider intuition important.
I believe that in the ideal world, intuition, &amp;quot;explanatory power&amp;quot; and
teachability would play a role that has weight equal or greater than
&amp;quot;new results&amp;quot; of research. This is clearly not the world we
live in, but we can nevertheless asipre.&lt;&#x2F;p&gt;
&lt;p&gt;So let me draw a rough &amp;quot;baseline&amp;quot; of logic, computation and programming languages.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Natural deduction (not sequent calculus) is the standard notation for
discussing formal reasoning, proof, structural and substructural proof theory. 
It goes back to Jaskowski and Gentzen, but today, two people deserve special mention:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dag Prawitz for picking up proof-theoretic semantics and natural 
deduction, providing a normalization theorem for natural deduction
calculi&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Per Martin-LÃ¶f for basing his presentation of &lt;a href=&quot;https:&#x2F;&#x2F;archive-pml.github.io&#x2F;martin-lof&#x2F;pdfs&#x2F;Bibliopolis-Book-retypeset-1984.pdf&quot;&gt;intuitionistic type
theory&lt;&#x2F;a&gt; on the natural deduction style and establishing the concept of 
judgment.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Type systems are presented as natural deduction calculi (with &amp;quot;localized assumptions&amp;quot;, so a judgment looks like a sequent of sequent calculus). Type systems provide the most effective form of mechanized reasoning and program analysis
that we have in programming languages. &lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;!--This is not even
mentioning the advantages for documentation, IDE support, modularity
and separate compilation. In the Curry-Howard perspective, type-checking is 
a strangely backwards operation where we have a proof and are looking
for the proposition it is proving.
--&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sequent calculus is the device used in more formal studies of reasoning, 
structured and substructual proof. When we use sequent calculus, we
accept a greater distance to intuition and &amp;quot;informal reasoning&amp;quot; for
technical reasons. Automated reasoning using &amp;quot;semantic tableaux&amp;quot; is
essentially the same as working with sequent calculus.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In &lt;a href=&quot;https:&#x2F;&#x2F;www.paultaylor.eu&#x2F;stable&#x2F;prot.pdf&quot;&gt;Proofs and Types&lt;&#x2F;a&gt;, Jean-Yves Girard argues that sequent calculus is &amp;quot;the prettiest illustration of
the symmetries of Logic&amp;quot;, and that it was &amp;quot;generally ignored by computer scientists.&amp;quot; This may have been true at the time, but it is certainly no longer the case.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Samson Abramsky does give the sequent calculus its deserved treatment
in &lt;a href=&quot;https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;030439759390181R&quot;&gt;Computational interpretations of linear logic&lt;&#x2F;a&gt;, though Girard may not have had an operational reading in mind. Abramsky describes a symmetry between
constructors (right rules, introduction rules) and destructors (left rules,
elimination rules) which gives a proof-theoretic explanation on
insights that go back to the McCarthy and Landin.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;focusing-and-uniform-proofs&quot;&gt;Focusing and uniform proofs&lt;&#x2F;h2&gt;
&lt;p&gt;Now that we have established a (possibly subjective) baseline, let&#x27;s talk
about focusing. Focusing makes it appearance in logic programming: here 
computation is not proof normalization, but proof &lt;strong&gt;search&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Dale Miller, Gopalan Nadathur, Frank Pfenning and Andre Scredrov. &amp;quot;Uniform proofs as the foundation for logic programming&amp;quot;. Annals of Pure and Applied Logic. 51:125-157, 1991&lt;&#x2F;li&gt;
&lt;li&gt;Around roughly the same time, Jean-Marc Andreoli published &amp;quot;Logic programming with focusing proofs in linear logic.&amp;quot;. Journal of Logic and Computation. 2(3):197-347 &lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Focusing is a &lt;strong&gt;structuring principle&lt;&#x2F;strong&gt; for proofs. It is a way to eliminate
the redundancy that is introduced when passing from natural deduction to
sequent calculus (Pfennings &lt;a href=&quot;https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~fp&#x2F;courses&#x2F;oregon-m10&#x2F;04-focusing.pdf&quot;&gt;lectures notes&lt;&#x2F;a&gt;). Girard was aware of
Andreoli&#x27;s work and referenced it &amp;quot;On the unity of logic&amp;quot;, introducing
&amp;quot;positive&amp;quot; and &amp;quot;negative&amp;quot; polarity as concepts.&lt;&#x2F;p&gt;
&lt;p&gt;The key idea is that if a rule of inference is &lt;strong&gt;invertible&lt;&#x2F;strong&gt;, it makes sense to
apply it directly and not look for other rules. When invertible rules
have priority over others, this reduces the large search space of proofs.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Polarized focusing&lt;&#x2F;strong&gt; goes further: in the context of proof search is about &amp;quot;combining
runs of connectives that are positive or negative, with explicit coercions
between runs. These coercions, written $\uparrow$ and $\downarrow$ are
called &lt;em&gt;shift operators&lt;&#x2F;em&gt;.&amp;quot; (Pfenning, ibid.) This is what we discussed
in this series.&lt;&#x2F;p&gt;
&lt;!--
## Applications of Linear Logic
* After pt3, I prepared a talk [&quot;Call-by-push-value&quot; and ownership](https:&#x2F;&#x2F;burakemir.ch&#x2F;odersky-fest-23&#x2F;) where I shared my intention to apply some of this 
content to formalize a part of Rust&#x27;s type system.

  * In short, it has been known for a long time that linear&#x2F;affine types 
and substructural logic can be used to model resource management. The
wikipedia article on 
[Bunched logic](https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bunched_logic#Applications)
mentions John C. Reynolds using an affine type theory in 1978. This is
a whopping 9 years before Girard published &quot;linear logic&quot;.

  * This is not the place to discuss details neither the subtleties how the 
&quot;no duplication&quot; linearity constraints is part of such substructural logics
nor what part of Rust&#x27;s types system actually matches the &quot;no duplication&quot;
of linear logic. However, I found out that (besides Phil Wadler&#x27;s numerous
papers on linear logic), Martin Oderky published a little-known paper on 
&quot;observers for linear logic&quot; which describes read-only access to linear
types as useful in programming, and which is very close to the short-lived, 
immutable, shared references we call &quot;borrows&quot; today.
     * The &quot;lifetime parameters&quot; are what research calls &quot;region variables&quot; and I think Tofte &amp;amp; Talpin&#x27;s &quot;region-based memory management&quot; may be a good
best reference (it is at least what I learnt in grad school). Lots of
people researched regions afterwards.



https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bunched_logic#Applications
 in types ing resources like memory. I am talking the 1970s.
John C. Reynolds in 1978 (via wikipedia)
There are
more and related approaches, such as Tofte &amp;amp; Talpin&#x27;s &quot;region-based memory
management&quot;, uniqueness types and more recent paper by Guillaume
Munch-Maccagnoni [&quot;Resource Polymorphism&quot;](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.02796)
which discusses the use even in a garbage-collected language.


 for the [OderskyFest 2023](https:&#x2F;&#x2F;burakemir.ch&#x2F;odersky-fest-23&#x2F;)

# Rewind: Logic and Computation

I want to start with a few big picture thoughts what keeps on bringing
me back to logic.

am gathering some thoughts on why polarity and focusing
holds promise towards identifying &quot;fundamental&quot; concepts in programming language research.
Even without the Curry-Howard perspective and logic, distinguishing between
data and computation makes immediate sense. Can we get our formal systems
to reflect this intuition?
--&gt;
&lt;h2 id=&quot;what-just-happened&quot;&gt;What just happened?&lt;&#x2F;h2&gt;
&lt;p&gt;We saw how call-by-push-value (CBPV) &amp;quot;separates&amp;quot; typed $\lambda$-calculus
into &lt;em&gt;value types&lt;&#x2F;em&gt; and &lt;em&gt;computation types&lt;&#x2F;em&gt;. We obtain a type system which can be
viewed as a natural deduction calculus. This view may seem slightly forced, since
operations like $\mathtt{thunk}$ and $\mathtt{force}$ that control evalution
do not seem very logical. But there are compelling reasons to have them!&lt;&#x2F;p&gt;
&lt;p&gt;If we consider the treatment of inference and assumptions in linear
logic, the familiar operators $\vee, \wedge$ separate into additive
versions $\oplus, \&amp;amp;$ as well as multiplicate versions  â $\otimes$.&lt;&#x2F;p&gt;
&lt;p&gt;Some of these types $\oplus, \otimes$ look like data type constructors,
and we called these positive or value types. For the others  â $\&amp;amp;$.
evaluation does not proceed until the environment demands it. These are
negative or computation types.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Going back to Bob Harper&#x27;s post &lt;a href=&quot;https:&#x2F;&#x2F;existentialtype.wordpress.com&#x2F;2012&#x2F;08&#x2F;25&#x2F;polarity-in-type-theory&#x2F;&quot;&gt;polarity in type theory&lt;&#x2F;a&gt;: positive types come 
with a &lt;em&gt;single&lt;&#x2F;em&gt; elimination rules which describe how the proof makes
sense of the constituent(s) of the proposition. The connective is
&lt;strong&gt;inductively defined&lt;&#x2F;strong&gt; in the sense that the introduction rule
completely determines what the elimination rule does. It &amp;quot;writes itself&amp;quot;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;On the other hand, negative types come with elimination rules that 
determine what the proofs are going to look like. They are &lt;strong&gt;coinductively
defined&lt;&#x2F;strong&gt; in the sense that &amp;quot;there is no commitment to the internal
structure of a proof&amp;quot;, anything that provides a way to apply
elimination rules is acceptable. These may be called &amp;quot;lazy types&amp;quot;,
computation is suspended. &lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;There is more to say here (concurrency and &amp;quot;par&amp;quot;, applications to
memory management, category theory) but that will have to happen
some other time.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;putting-it-all-together&quot;&gt;Putting it all together&lt;&#x2F;h2&gt;
&lt;p&gt;Types can be defined by either introduction or elimination rules, and
the straightforward way to interpret this is to think of negative
types as suspending evaluation (&amp;quot;lazy&amp;quot;, unevaluated, &amp;quot;objects&amp;quot;) and
positive types as being fully evaluated (&amp;quot;structured data&amp;quot;).&lt;&#x2F;p&gt;
&lt;p&gt;Having both of these in a single calculus, that moreover corresponds to
polarized natural deduction, is simply amazing. Surely, being able to
encode various formal calculi that are either call-by-name or call-by-value
is interesting. From a more practical perspective, it is now easy to
see that when one wants to formalize &amp;quot;object-oriented style,&amp;quot; which will
decidedly involve something like records on unevaluated functions, this
will involve negative types.&lt;&#x2F;p&gt;
&lt;p&gt;To give a vauge, but hopefully illuminating idea: consider how a &amp;quot;vtable&amp;quot; 
is quite literally a record of function pointers, &amp;quot;waiting&amp;quot; for
program execution to select one among them before proceeding with
computation. And how representing an object involves carrying around a
vtable.&lt;&#x2F;p&gt;
&lt;p&gt;All this makes me think that when formalizing a programming language, it seems
very attractive to translate the calculus to CBPV (or something like CBPV).
Of course, this supposes that formalization is taking place - it looks
like I will finally have some motivation to learn a proof assistant.&lt;&#x2F;p&gt;
&lt;p&gt;I hope you found this little series useful in developing intuition for
linear logic as well as the role of &amp;quot;negative types.&amp;quot;&lt;&#x2F;p&gt;
</content>
    </entry>
    <entry xml:lang="en">
        <title>CBPV and Natural Deduction - Part 3. Linear Logic</title>
        <published>2023-08-25T00:00:00+00:00</published>
        <updated>2023-08-25T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://burakemir.ch/post/cbpv-pt3-linear-logic/" type="text/html"/>
        <id>https://burakemir.ch/post/cbpv-pt3-linear-logic/</id>
        <content type="html">&lt;p&gt;Welcome back to our study of polarized natural deduction and what 
CBPV has to do with it. For the previous parts 
of this journey, check out &lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt1-small-steps&#x2F;&quot;&gt;&amp;quot;pt1. small steps&amp;quot;&lt;&#x2F;a&gt;
and &lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt2-sum-product&#x2F;&quot;&gt;&amp;quot;pt2. sums and products&amp;quot;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-happened-so-far&quot;&gt;What happened so far&lt;&#x2F;h2&gt;
&lt;p&gt;In part 1, we introduced the essential parts of call-by-push-value (CBPV):
splitting types into value and computation types, with shifts between
these worlds. We saw that static typing constrains evaluation order,
without the need for an external specification like evaluation context
definition, and that this enables us to give an abstract machine.
Although the CK machine still uses substitution, the fact that it
only looks at bounded depth from the root makes it already a lot
more efficient than a recursive evaluator that has to descend
a term and looks for redexes. We hinted at the connection with
intermediate representation in compilers, given that all
intermediate results are named.&lt;&#x2F;p&gt;
&lt;p&gt;In part 2, we added sum and products tyes as value types, and also a
second product type as computation type. We hinted that the difference 
between the two product types has to do with linear logic.&lt;&#x2F;p&gt;
&lt;p&gt;In this part, we reveal how linear logic relates to these two
different kind of product types. The very notion of propositions having 
polarity comes from linear logic; going in this direction is
a deliberate choice in order to simple, intuitive explanation.&lt;&#x2F;p&gt;
&lt;p&gt;Another choice to discuss CBPV is to focus on natural deduction.
Almost all linear logic discourse happens on sequent calculi, but
Prawitz that natural deduction is illuminating, as witnessed by
the fact that the simplest example of Curry-Howard-Lambek
correspondence is between $\lambda$ calculus and natural deduction
for minimal logic.&lt;&#x2F;p&gt;
&lt;p&gt;Let us dive right in and get from natural deduction to linear natural deduction.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;assumptions-and-linearity&quot;&gt;Assumptions and Linearity&lt;&#x2F;h2&gt;
&lt;p&gt;We formulated our typing rules in the standard way, with judgments
that read $\Gamma \vdash e : T$. In terms of logic, this is a
hypothetical judgment &amp;quot;$T$ is true under assumptions $\Gamma$&amp;quot;.&lt;&#x2F;p&gt;
&lt;p&gt;This style of writing judgments is called &amp;quot;localized assumptions&amp;quot; or 
sometimes sequent style: all available assumptions are repeated on 
every node of our derivation. Most treatments of natural deduction
instead assumptions as some leaf nodes that are marked when
they are discharged.&lt;&#x2F;p&gt;
&lt;p&gt;In the programming view, assumptions are assignments of a type to a variable. 
In using the $\lambda$-calculus as a programming language, we can use
variables in an unconstrained maner, as in these expressions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathrm{twice} := \lambda f: A \rightarrow A. \lambda x: A. f (f x)$ and $\mathrm{double} := \lambda x: Int. x + x$&lt;&#x2F;li&gt;
&lt;li&gt;$\mathrm{K} := \lambda x. \lambda y. x$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Type derivations for the bodies $f (f x)$ and $x + x$ and $x$ will use variables
from the typing context more than once. In the case of $K$, there will be 
variable $y$ introduced to the context that is not used at all.
These examples show that we use $\Gamma$ as a set of assumptions. We can freely
use anything that is in the set.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Linearity&lt;&#x2F;em&gt; use a variable exactly once. Just as in mathematics, a linear equation has 
the shape $a * x + b = 0$ where $x$ is used once (with $a \neq 0$, thus polynomial degree 1), in &lt;em&gt;linear logic&lt;&#x2F;em&gt;,
we are interested in the case that an assumption is used exactly once. 
Why would we do this? There are many concrete applications to linearity
in programming languages. The one we will be most interested in is the 
inherent notion of &lt;em&gt;exclusive ownership&lt;&#x2F;em&gt; that comes from treating types
as resources.&lt;&#x2F;p&gt;
&lt;p&gt;A close friend of linearity is &lt;em&gt;affinity&lt;&#x2F;em&gt; where polynomial degree may
be either 1 or 0. We can ignore variables. This is pretty interesting as
you can see from the examples: with an affine type system, we can still
assign a type to terms like $K$ combinator that ignore variables.
The more practical example is code like this:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#383838;color:#e6e1dc;&quot;&gt;&lt;code&gt;&lt;span&gt;fn (x: &amp;amp;mut u32) {
&lt;&#x2F;span&gt;&lt;span&gt;  if (some_condition()) { *x = 1; }
&lt;&#x2F;span&gt;&lt;span&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;!--
There is more to say about linear logic than these applications, but for 
our purposes, understanding CBPV as polarized natural deduction (with an
application to tracking exclusive ownership) this is sufficient.
--&gt;
&lt;!--
Note that if a program was already linear, our rules would already be 
good for typing it. We now want to refine our typing rules so that only
linear programs, to exclude the possibility
of using a variable more than once.
--&gt;
&lt;!--
We can still consider the context to be a set; we just need to careful
when the rules combine typing contexts. We want to make sure that contexts
are disjoint sets.
--&gt;
&lt;!--
Consequently, the expressions above will not be typable. The types come with
an inherent ownership: they are resource, and - in modern programming terms -
a $\lambda$ expression has *ownership* of these resources. When we combine
derivations, we make explicit that they do not share any assumptions.
Let&#x27;s jump right in.
--&gt;
&lt;h2 id=&quot;linear-cbpv&quot;&gt;Linear CBPV&lt;&#x2F;h2&gt;
&lt;p&gt;We can now define a linear natural deduction calculus, using the standard notation for
linear logic connectives. We will do this by simply repeating all the rules
we had previously for call-by-push-value, but now with the added constraint
that variables (assumptions) are used exactly once. For the judgment
$\Gamma \vdash e: T$ we require that exactly the free variables in $e$ appear in $\Gamma$.
If we want an affine type discipline instead, we require that $\Gamma$ be a superset
of all the variables in $e$.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$x : A^+ \in \Gamma$}
\RightLabel{$(\mathrm{hyp})$}
\UnaryInfC{$\Gamma \vdash x: A^+$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Introduction and elimination rules for &lt;em&gt;linear implication&lt;&#x2F;em&gt; $\multimap$.
Note how we use $\Delta$ to emphasize that variables are disjoint from $\Gamma$.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma, x: A^+ \vdash s: B^-$}
\RightLabel{$(\multimap_I)$}
\UnaryInfC{$\Gamma \vdash (\lambda x: A^+. s):\, A^+ \multimap B^-$}
\end{prooftree}
\quad
\quad
\begin{prooftree}
\AxiomC{$\Gamma \vdash s:\, A^+ \multimap B^-$}
\AxiomC{$\Delta \vdash t:\, A^+$}
\RightLabel{$(\multimap_E)$}
\BinaryInfC{$\Gamma, \Delta \vdash (s\ t):\, B^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Linear implication $A^+ \multimap B^-$ is different from arrow type: 
the argument resource $A^+$ is &lt;em&gt;consumed&lt;&#x2F;em&gt; and in return we get a certain
computation resource $B^-$.&lt;&#x2F;p&gt;
&lt;p&gt;Now comes the shift from values to computations. As before, if you
compare with &lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt1-small-steps&#x2F;&quot;&gt;part 1&lt;&#x2F;a&gt;, the only difference 
is that there is a separate, disjoint context named $\Delta$.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma \vdash s: A^+$}
\RightLabel{$(\uparrow_I)$}
\UnaryInfC{$\Gamma \vdash \mathtt{return}\ s:\, \uparrow\!{}A^+$}
\end{prooftree}
\quad
\quad
\begin{prooftree}
\AxiomC{$\Gamma \vdash s:\, \uparrow\!{}A^+$}
\AxiomC{$\Delta, x: A^+ \vdash t:\, C^-$}
\RightLabel{$(\uparrow_E)$}
\BinaryInfC{$\Gamma, \Delta \vdash \mathtt{let\ val}\ x = s\ \mathtt{in}\ t:\, C^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;This is easy! The rules for introducing $(\downarrow_I)$ and eliminating
$(\downarrow_E)$ the shift from computation to values do not actually change so
we don&#x27;t repeat them here (consult &lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt1-small-steps&#x2F;&quot;&gt;part 1&lt;&#x2F;a&gt;).
We have defined a core linear CBPV calculus with abstraction and application.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sums-and-products&quot;&gt;Sums and Products&lt;&#x2F;h2&gt;
&lt;p&gt;We move on to sum and product types discussed in &lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt2-sum-product&#x2F;&quot;&gt;part 2&lt;&#x2F;a&gt;. There,
we noticed two distinct product types: a value pair type and a computation record type.&lt;&#x2F;p&gt;
&lt;p&gt;The sum and product value types correspond to disjunction and conjuction in minimal and intuitionistic 
logic. In linear logic, there are multiple ways to define disjunction. We use standard, but different symbols 
for the corresponding linear logic connectives.&lt;&#x2F;p&gt;
&lt;p&gt;The linear version of our sum value type correspons to &lt;em&gt;additive disjunction&lt;&#x2F;em&gt; $\oplus$ (still called &amp;quot;plus&amp;quot;):
$$
\begin{prooftree}
\AxiomC{$\Gamma\vdash v: A_j^+$}
\RightLabel{$(\oplus_I)$}
\UnaryInfC{$\Gamma \vdash \mathtt{inj}_i\ v : {\color{lightgreen}{{\large \oplus}_I A^+_i}}$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma \vdash v : {\color{\lightgreen}{{\large \oplus}_I A^+_i}}$}
\AxiomC{$\ldots \Delta_i, x: A_i^+ \vdash M_i : B^- \ldots$}
\RightLabel{$(\oplus_E)$}
\BinaryInfC{$\Gamma \ldots \Delta_i \ldots \vdash \mathtt{match}\ V\ \{\ldots, \mathtt{case}\ i: x.M_i \ldots\}: B^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Our pair value type, as hinted at in the last part, is &lt;em&gt;multiplicative conjunction&lt;&#x2F;em&gt; $\otimes$ (&amp;quot;tensor&amp;quot;): we cannot
use a component of a pair and drop the other one, we have to use both.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma\vdash v_1: A_1^+$}
\AxiomC{$\Delta\vdash v_2: A_2^+$}
\RightLabel{$(\otimes_I)$}
\BinaryInfC{$\Gamma, \Delta \vdash (v_1, v_2) :\, {\color{lightgreen}{A_1^+ \otimes A_2^+}}$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma\vdash v: {\color{\lightgreen}{A_1^+ \otimes A_2^+}}$}
\AxiomC{$\Delta, x: A_1^+, y: A_2^+ \vdash M: B^-$}
\RightLabel{$(\otimes_E)$}
\BinaryInfC{$\Gamma, \Delta \vdash \mathtt{match}\ V\ \mathtt{as}\ (x, y).M : B^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Finally, let us look at the &amp;quot;lazy&amp;quot; (unevaluated) pair computation type. This connective
is called additive conjunction and written $\&amp;amp;$ (pronounced &amp;quot;with&amp;quot;).
For a linear record of computation, we cannot use multiple fields, we can pick one, 
losing access to the rest.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\ldots \Gamma_i \vdash s_i: B_i^- \ldots$}
\RightLabel{$({\large \&amp;amp;}_I)$}
\UnaryInfC{$\ldots \Gamma_i \ldots \vdash \lambda\{\ldots i.s_i\ldots \}: {\color{lightblue}{{\large \&amp;amp;}_I\ i.B_i^-}}$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma\vdash s : {\color{lightblue}{{\large \&amp;amp;}_{i \in I}\ i.B_i^-}}$}
\RightLabel{$({\large \&amp;amp;}_E)$}
\UnaryInfC{$\Gamma \vdash s\ \mathtt{get}\ i: B_i^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;We have defined a linear CBPV calculus with &amp;quot;plus&amp;quot; $\oplus$ (additive disjunction), 
&amp;quot;tensor&amp;quot; $\otimes$ (multiplicative conjuction) and &amp;quot;with&amp;quot; $\&amp;amp;$ (additive conjunction) .&lt;&#x2F;p&gt;
&lt;p&gt;At this point, you may wonder about the missing combination: multiplicative disjunction. This
is written â (pronounced &amp;quot;par&amp;quot;) and corresponds to a sort of separate composition (say, 
parallel composition of processes).  Other notions of composition along two dimension work,
too, for instance space and time (fun fact that concurrency is &amp;quot;NebenlÃ¤ufigkeit&amp;quot; in German,
which &amp;quot;side-by-side running&amp;quot;).  All this and more linear logic concepts would lead us slightly
too far away from our series topic. &lt;&#x2F;p&gt;
&lt;h2 id=&quot;towards-an-understanding-of-polarity&quot;&gt;Towards an understanding of polarity&lt;&#x2F;h2&gt;
&lt;p&gt;In our journey to understand polarity in natural deduction, we have arrived at a happy place that
lets us see a clear and easy to remember way to relate computation to CBPV, understood
as the analogon of an intermediate representation in a compiler. The order of evaluation
is specified without the help of external means.&lt;&#x2F;p&gt;
&lt;p&gt;In the next and final part, we shall look at polarity in more detail and discuss possible
applications for programming language implementation.&lt;&#x2F;p&gt;
</content>
    </entry>
    <entry xml:lang="en">
        <title>CBPV and Natural Deduction - Part 2. Sums and Products</title>
        <published>2023-08-20T00:00:00+00:00</published>
        <updated>2023-08-20T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://burakemir.ch/post/cbpv-pt2-sum-product/" type="text/html"/>
        <id>https://burakemir.ch/post/cbpv-pt2-sum-product/</id>
        <content type="html">&lt;p&gt;We are continuing our look at call-by-push-value (CBPV), natural deduction and
abstract machines. &lt;a href=&quot;https:&#x2F;&#x2F;burakemir.ch&#x2F;post&#x2F;cbpv-pt1-small-steps&#x2F;&quot;&gt;Last time&lt;&#x2F;a&gt; we looked
at a bare-bones version so we could focus on $\lambda$-abstraction and
application.&lt;&#x2F;p&gt;
&lt;p&gt;In this part, we will only add sum and product types. In the next part we can
then shift our view towards linear logic concepts and resources.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sum-and-product-types&quot;&gt;Sum and Product Types&lt;&#x2F;h2&gt;
&lt;p&gt;We add a sum type $ \Sigma A_i $ and a product type $A_1 \times A_2$ to our value types. We also add lazy products,
$\Pi_{i \in I}\ i.B_i^-$ for reasons that have to do with the fine-grained distinction between
value and computation types.  The presentation in this section follows Levy&#x27;s book closely, except
minor changes in syntax.&lt;&#x2F;p&gt;
&lt;!-- par &amp;#x214B; --&gt;
&lt;p&gt;$$
\begin{array}{lll}
A^+ &amp;amp;::= &amp;amp; \mathbf{1}\ |\ {\color{lightgreen}{\Sigma_{i \in I} A_i}}\ |\ {\color{lightgreen}{A_1^+ \times A_2^+}}\ |\ \downarrow{A^-} \\
B^- &amp;amp;::= &amp;amp; A^+ \rightarrow B^-\ |\ {\color{lightblue}{\Pi_{i \in I}\ i.B_i^-}}\ |\ \uparrow{A^+} 
\end{array}
$$&lt;&#x2F;p&gt;
&lt;p&gt;By looking at the introduction and elimination rules, we can get intuition what  sum types are.
can construct value of sum type by &lt;em&gt;injecting&lt;&#x2F;em&gt;, and when we have a value of sum type,
we can perform a case-distinction and recover which injector was used. We use a $\mathtt{match}$
syntax for the case distinction. The only special thing for CBPV is that sum types are
made from value types, they are themselves value types, but a match is a computation and
therefore yields a computation type.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma\vdash v: A_j^+$}
\RightLabel{$(+_I)$}
\UnaryInfC{$\Gamma \vdash \mathtt{inj}_i\ v : {\color{lightgreen}{\Sigma_I A^+_i}}$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma \vdash v : {\color{\lightgreen}{\Sigma_I A^+_i}}$}
\AxiomC{$\ldots \Gamma, x: A_i^+ \vdash M_i : B^- \ldots$}
\RightLabel{$(+_E)$}
\BinaryInfC{$\Gamma \vdash \mathtt{match}\ V\ \{\ldots, \mathtt{case}\ i: x.M_i \ldots\}: B^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;We do the same with a pair type: using pair syntax constructs a pair, both components
have to be value types and the pair itself is a value type. Elimination is interesting:
we require that both values are bound at the same time. The reason for this is that
in CBPV, projection would be a computation.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma\vdash v_1: A_1^+$}
\AxiomC{$\Gamma\vdash v_2: A_2^+$}
\RightLabel{$(\times_I)$}
\BinaryInfC{$\Gamma \vdash (v_1, v_2) :\, {\color{lightgreen}{A_1^+ \times A_2^+}}$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma\vdash v: {\color{\lightgreen}{A_1^+ \times A_2^+}}$}
\AxiomC{$\Gamma, x: A_1^+, y: A_2^+ \vdash M: B^-$}
\RightLabel{$(\times_E)$}
\BinaryInfC{$\Gamma \vdash \mathtt{match}\ V\ \mathtt{as}\ (x, y).M : B^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Taken together, the value types for sums and products are enough to give us sum-of-product algebraic datatypes,
though pattern matching operates only on one level at a time.&lt;&#x2F;p&gt;
&lt;p&gt;In terms of logic, sums are disjunction and pairs are conjunction. Defining pairs 
in this way is a harbinger of what is to follow in the next part: Bob Harper says one
might as well write $v_1 \otimes v_2$ (tensor) in his post
on &lt;a href=&quot;https:&#x2F;&#x2F;existentialtype.wordpress.com&#x2F;2012&#x2F;08&#x2F;25&#x2F;polarity-in-type-theory&#x2F;&quot;&gt;polarity in type theory&lt;&#x2F;a&gt;,
but admits it is pointless if we are not doing linear logic.
We leave this discussion of linear logic to the next part.&lt;&#x2F;p&gt;
&lt;p&gt;Now we come to the &amp;quot;lazy product&amp;quot;, which is closer in spirit to records of functions (which could
be called &amp;quot;objects&amp;quot;, but this is simplifying objects a lot). We bundle suspended computations in a
record, and we have a computation to select a particular &amp;quot;field&amp;quot; from this record.&lt;&#x2F;p&gt;
&lt;!--\UnaryInfC{$\Gamma \vdash \lambda\\{\\} :\\, $}--&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\ldots \Gamma \vdash s_i: B_i^- \ldots$}
\RightLabel{$(\Pi_I)$}
\UnaryInfC{$\Gamma \vdash \lambda\{\ldots i.s_i\ldots \}: {\color{lightblue}{\Pi_I\ i.B_i^-}}$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma\vdash s : {\color{lightblue}{\Pi_{i \in I}\ i.B_i^-}}$}
\RightLabel{$(\Pi_E)$}
\UnaryInfC{$\Gamma \vdash s\ \mathtt{get}\ i: B_i^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Also here, we shall leave the corresponding linear logic connector, &amp;amp; &amp;quot;with&amp;quot;, for the next part.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;machine-transitions&quot;&gt;Machine Transitions&lt;&#x2F;h2&gt;
&lt;p&gt;We extend the CK machine accordingly. There is a new stack frame $(\_ \mathtt{get}\ i)$, otherwise
the transitions are as one would expect them. As before, the heavy lifting in CK machine is done 
by substitution.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{array}{llll}
C                                        &amp;amp; K &amp;amp; \rightsquigarrow &amp;amp; C&#x27; &amp;amp; K&#x27; \\
s\ \mathtt{get}\ i         &amp;amp; k &amp;amp; &amp;amp; s   &amp;amp;  (\_ \mathtt{get}\ i)::k \\
\lambda\{\ldots i.s_i\ldots \}         &amp;amp; (\_ \mathtt{get}\ i)::k &amp;amp; &amp;amp; s_i   &amp;amp;  k \\
\mathtt{match} (v_1, v_2)\ \mathtt{as}\ (x, y).M  &amp;amp; k &amp;amp;                 &amp;amp; M[x := v_1, y:= v_2]  &amp;amp; k \\
\mathtt{match} (\mathtt{inj}\ i\ v) \{ \ldots \mathtt{case}\ i: x.M_i\ldots \} &amp;amp; k &amp;amp;                 &amp;amp; M_i [x:=v] &amp;amp; k \\
\end{array}
$$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;preview-of-the-next-part&quot;&gt;Preview of the next part&lt;&#x2F;h2&gt;
&lt;p&gt;We have discussed CBPV following a narrative of polarized natural deduction. There
were some previews of linear logic discussion, but fundamentally, the types could
not yet be interpreted as linear logic proposition.&lt;&#x2F;p&gt;
&lt;p&gt;The reason for this is that in natural deduction, assumptions can be used multiple 
times and discharged whenever we want. This corresponds to the structural rules of 
sequent calculus, of which Girard says they are the most important of them all.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$x : A^+ \in \Gamma$}
\RightLabel{$(\mathrm{hyp})$}
\UnaryInfC{$\Gamma \vdash x: A^+$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Therefore, if we want to set up linear natural deduction, we have to first
make precise where and how this unconstrained use of hypotheses takes place.
We can then see how to change perspective and decree that assumptions can be
used exactly once (linear) or up to once (affine). Stay tuned for the next
session.&lt;&#x2F;p&gt;
</content>
    </entry>
    <entry xml:lang="en">
        <title>CBPV and Natural Deduction - Part 1. Small steps</title>
        <published>2023-07-22T00:00:00+00:00</published>
        <updated>2023-07-22T00:00:00+00:00</updated>
        <author>
          <name>Unknown</name>
        </author>
        <link rel="alternate" href="https://burakemir.ch/post/cbpv-pt1-small-steps/" type="text/html"/>
        <id>https://burakemir.ch/post/cbpv-pt1-small-steps/</id>
        <content type="html">&lt;p&gt;I have been reading up on call-by-push-value (CBPV),
a variation of $\lambda$-calculus that is more
fine-grained in its treatment of argument-passing.
I then came across a chapter
from Frank Pfenning&#x27;s 2016 &lt;a href=&quot;https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~fp&#x2F;courses&#x2F;15816-f16&#x2F;schedule.html&quot;&gt;lecture notes on substructual logic&lt;&#x2F;a&gt; that 
characterizes CBPV as &lt;em&gt;polarized natural deduction.&lt;&#x2F;em&gt; In a small series
of posts, I am exploring what this means and how this relates to
compilers.&lt;&#x2F;p&gt;
&lt;p&gt;The Curry-Howard-Lambek correspondence is a nice way to connect logic and computation. 
I have written about this in previous posts on my old blog, see
&lt;a href=&quot;https:&#x2F;&#x2F;bq9.blogspot.com&#x2F;2020&#x2F;04&#x2F;higher-order-logic-and-equality.html&quot;&gt;Higher-order logic and equality&lt;&#x2F;a&gt;
and
&lt;a href=&quot;https:&#x2F;&#x2F;bq9.blogspot.com&#x2F;2020&#x2F;05&#x2F;intuitionistic-propositional-logic-and.html&quot;&gt;Intuitionistic Propositional Logic and Natural Deduction&lt;&#x2F;a&gt;.
For the first part, those posts should be enough to tag along.
In the next parts, we may look at some fine points of natural deduction.&lt;&#x2F;p&gt;
&lt;!--
we sequential composition
Now CBPV raises a question:
A few reasons: 
* What does this &quot;finer-grained treatment of passing arguments&quot; correspond to
in this logical perspective? 
* if we instead regard $\lambda$-calculus as a model of computation, there are
also *abstract machines* which would be more fine-grained models of computation.
Does CBPV yield insights that we can put to good use when passing from
the $\lambda$-calculus to the lower-level machine formalisms? Could these
insights help us with the design of intermediate representations (IR) for
compilers that would support reasoning about correctness of program
transformations and optimizations?
--&gt;
&lt;!--
For the second perspective, there will be a follow up at some point. For
a sneak preview, consider the a paper [Structural Operational Semantics for Control Flow Graph Machines](https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1805.05400&#x27;) by Dmitri Garbuzov, William Mansky, Christine Rizkallah, Steve Zdancewic.
--&gt;
&lt;h1 id=&quot;reduction-and-strategy&quot;&gt;Reduction and strategy&lt;&#x2F;h1&gt;
&lt;p&gt;In $\lambda$-calculus, our elementary computation step is a $\beta$-reduction. 
Taking the perspective of the Curry-Howard-Lambek correspondence, we can regard 
a type as a logical proposition and a term of typed $\lambda$-calculus as a proof 
of this proposition. Here, $\beta$-reduction is a rewriting
of the proof, which removes a &#x27;detour&#x27; in the proof. In other words, computation is proof normalization.&lt;&#x2F;p&gt;
&lt;p&gt;$$
( \lambda x : A .  M)\ N \longrightarrow_Î² M [ x := N ]
$$&lt;&#x2F;p&gt;
&lt;p&gt;Here, $[ x := N ]$ is our way of writing substitution. The full definition requires
the usual careful treatment of bound names. The rewriting can take place anywhere in
the term.&lt;&#x2F;p&gt;
&lt;!--
Resource-efficient programming language implementations require
*lowering* substitution and symbolic representation, i.e. find lower-level representations 
that have same meaning (operational semantics). But let&#x27;s not get ahead of ourselves:
we should first have an operational semantics.
--&gt;
&lt;p&gt;Suppose we wanted to specify the &lt;em&gt;order&lt;&#x2F;em&gt; of computation steps. When there are not one, but several reducible expressions,
which reduction should happen first?&lt;&#x2F;p&gt;
&lt;p&gt;A small-step semantics would specify not only reduction, but also where exactly evaluation takes place.
To this end, one defines &lt;em&gt;evaluation context&lt;&#x2F;em&gt;, a term with a hole that specifies exactly
where in the term reduction is permitted to happen. Here is a sample definition, where we consider
$\lambda$-abstractions as values (we do not reduce under a $\lambda$.)&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{array}{lll}
E &amp;amp;::= &amp;amp;x\ |\ \lambda x. E\ |\ E\ E \\
C &amp;amp;::= &amp;amp;[~]\ |\ E\ C\ |\ C\ V \\
V &amp;amp;::= &amp;amp;\lambda x. E
\end{array}
$$&lt;&#x2F;p&gt;
&lt;p&gt;When we have a context, we can fill its hole with a $\lambda$-expression and now exactly
where evaluation takes place. The hole of a context could also be filled with another
context.&lt;&#x2F;p&gt;
&lt;p&gt;The definition of evaluation contexts forces the argument (operand) in
an application to be evaluated before the operator. The example above is thus a form
of call-by-value. Note that the definition of $C$ comes with a somewhat arbitrary decision to evaluate the operand
before the operator. For call-by-name, passing a term that is not value would
require a context like $[~]\ E$. &lt;&#x2F;p&gt;
&lt;p&gt;Our specification of evaluation contexts seems to impose order by
defining what form of argument-passing is possible; in other
words, constraining (or leaving unconstrained) what can be bound to an identifier.&lt;&#x2F;p&gt;
&lt;!--
This brings us to a fundamental question: what can we bind to a variable? We
may want to require that an argument of a $\lambda$-expression
has to be reduced as much as possible before it is being bound to a
variable. Or we may say, let any term be bound, and let the reduction 
happen later when we cannot delay it further.
This is, very roughly, the difference of call-by-value vs call-by-name.

Another fundamental question is: how much do we reduce? Specifically,
if we have a $\lambda$ expression, do we keep looking for reducible
expressions? In many applications it is fine to stop reducing when
a term has become a $\lambda$-expression, but when we talk about
normal form of a proof, we want to reach a form where all reductions
have been carried out and no more are possible.
--&gt;
&lt;h1 id=&quot;values-and-computations&quot;&gt;Values and computations&lt;&#x2F;h1&gt;
&lt;p&gt;Instead of defining evaluation contexts separately, we now
look at a different way of impose order on evaluation. 
In doing so, we start treating contexts as computation (filling the hole
with a value is something that can produce a new value). We will see that 
what we are about to do is not very different from specifying an
intermediate representation (IR) of a compiler.&lt;&#x2F;p&gt;
&lt;p&gt;CBPV is a calculus that encompasses both call-by-value and call-by-name.
It achieves this through a fine-grained distinction between terms that are
&lt;em&gt;values&lt;&#x2F;em&gt; vs terms that are &lt;em&gt;computations&lt;&#x2F;em&gt; which is enfored by a
type disciple. Therefore we will have value types 
$A^+$ and computation types $B^-$:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{array}{lll}
B^- &amp;amp;::= &amp;amp; A^+ \rightarrow B^-\ |\ \uparrow{A^+} \\
A^+ &amp;amp;::= &amp;amp;\mathbf{1}\ |\ \downarrow{B^-}
\end{array}
$$&lt;&#x2F;p&gt;
&lt;p&gt;The type operators $\uparrow{}$ and $\downarrow{}$ are described
below. We add $\mathbf{1}$ (the &amp;quot;unit type&amp;quot;) as a base type, with 
only inhabitant $\mathtt{()}$
that we pronounce as &amp;quot;unit&amp;quot;.
Base types like $\mathtt{Int}$
or $\mathtt{String}$ would also be value types. All variables
have value type.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{}
\RightLabel{$(1_I)$}
\UnaryInfC{$\cdot \vdash (): \mathbf{1}$}
\end{prooftree}
\quad
\quad
\begin{prooftree}
\AxiomC{$x : A^+ \in \Gamma$}
\RightLabel{(hyp)}
\UnaryInfC{$\Gamma \vdash x: A^+$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;The arrow type
forces arguments to be of value type and the result to be of
computation type. What matters most is the interplay
of $\lambda$-abstraction and application. 
Here are the typing rules for these:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma, x: A^+ \vdash M: B^-$}
\RightLabel{$(\to_I)$}
\UnaryInfC{$\Gamma \vdash (\lambda x: A^+. M):\, A^+ \rightarrow B^-$}
\end{prooftree}
\quad
\quad
\begin{prooftree}
\AxiomC{$\Gamma \vdash M:\, A^+ \rightarrow B^-$}
\AxiomC{$\Gamma \vdash V:\, A^+$}
\RightLabel{$(\to_E)$}
\BinaryInfC{$\Gamma \vdash (M\ V):\, B^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;In words, a $\lambda$-expression has an arrow type which is a computation (negative) type.
We can chain abstractions like $\lambda x:X^+. \lambda y:Y^+. M$ for some
term $M$ of computation type, but the argument types $X^+, Y^+$ are forced
to be be value (positive) types. Application yields a computation type.
This is where it may be useful to remember that filling the hole of a context
yields something that we can turn into a value (but it is not a value yet).&lt;&#x2F;p&gt;
&lt;p&gt;You may have noticed the $I$ and $E$ letters in the rule names.
In natural deduction, every logical connective comes
with an introduction and elimination rule. Even though we write
these like sequents of sequent calculus, there are a few differences.
In sequent calculus, there are left- and
right-(introduction)-rules, and a computation step corresponds to 
the removal of detours (lemmas) via cut-elimination. &lt;&#x2F;p&gt;
&lt;h3 id=&quot;shifts&quot;&gt;Shifts&lt;&#x2F;h3&gt;
&lt;p&gt;As any type discipline, the above rules make sure that certain programs
cannot be written anymore. 
How can we return a value, though? Or write an identity computation? Or 
pass a function as an argument to another function? Our calculus is
not yet complete.&lt;&#x2F;p&gt;
&lt;p&gt;First, we need a way to turn a value into a computation. More precisely,
we want to turn a term $V: A^+$ of value (positive) type into a term
of computation (negative) type.&lt;&#x2F;p&gt;
&lt;p&gt;Let us call this operation $\mathtt{return}\ p$. This is a &amp;quot;shift&amp;quot; between
value types and computation types and is made
explicit using a type operator $\uparrow\!{}A^+$. The notation requires
some decoding work since $\uparrow\!{}A^+$ is a computation (negative) type.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma \vdash V: A^+$}
\RightLabel{$(\uparrow_I)$}
\UnaryInfC{$\Gamma \vdash \mathtt{return}\ V:\, \uparrow\!{}A^+$}
\end{prooftree}
\quad
\quad
\begin{prooftree}
\AxiomC{$\Gamma \vdash s:\, \uparrow\!{}A^+$}
\AxiomC{$\Gamma, x: A^+ \vdash M:\, B^-$}
\RightLabel{$(\uparrow_E)$}
\BinaryInfC{$\Gamma \vdash \mathtt{let\ val}\ x = M\ \mathtt{in}\ N:\, B^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;The corresponding elimination operation takes a suspended computation and yields 
a value. We use a &amp;quot;$\mathtt{let\ val}$&amp;quot; declaration as source syntax. Note how the type 
discipline forces the righthand-side to be a suspended computation and how this
imposes an order - we need to have the value before continuing. In turning the
the right-hand side into a value, we not only know where the actual computation
happens; we also bind the result to a local name.&lt;&#x2F;p&gt;
&lt;p&gt;This amounts to all intermediary results being named.&lt;&#x2F;p&gt;
&lt;p&gt;Next, we want to &amp;quot;package&amp;quot; a computation
into a value (&lt;em&gt;suspend&lt;&#x2F;em&gt; the computation). This will let us pass a $\lambda$-abstraction
as an argument to another $\lambda$-abstraction. We introduce an operator 
$\mathtt{thunk}\ t$ that suspends a computation and an operator 
$\mathtt{force}\ s$ that resumes a suspended computation.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{prooftree}
\AxiomC{$\Gamma \vdash M: B^-$}
\RightLabel{$(\downarrow_I)$}
\UnaryInfC{$\Gamma \vdash \mathtt{thunk}\ M:\, \downarrow\!{}B^-$}
\end{prooftree}
\quad
\quad
\begin{prooftree}
\AxiomC{$\Gamma \vdash V:\, \downarrow\!{}B^-$}
\RightLabel{$(\downarrow_E)$}
\UnaryInfC{$\Gamma \vdash \mathtt{force}\ V:\, B^-$}
\end{prooftree}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Again, the shift operator indicates in the type that we 
have a suspended computation which is a value. Thus,
$\downarrow$ shifts a negative to a positive type, and
$\downarrow\!A^-$ can be used in all places that
require a value type.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;example-programs&quot;&gt;Example programs&lt;&#x2F;h3&gt;
&lt;p&gt;Let&#x27;s look at examples, starting with the identity combinator
of plain $\lambda$-calculus. We want one of these at every type $I_A := \lambda x: A. x$ of type
$A \rightarrow A$. In the above polarized $\lambda$-calculus,
we get something close enough:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;for value types, there is: $\mathit{idval}_{A^+} := \lambda x: A^+. \mathtt{return}\,x$&lt;&#x2F;li&gt;
&lt;li&gt;for computation types, we have: $\mathit{idcmp}_{B^-} := \lambda x:\,\downarrow{B^-}. \mathtt{force}\ x$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;(Exercise: what are the types of these? What kind of &amp;quot;optimization&amp;quot; would remove these?)&lt;&#x2F;p&gt;
&lt;!-- By looking at the rules, we can verify that this has type $A^+ \rightarrow\\,\uparrow\\!{}A^+$. --&gt;
&lt;!--
If we want to pass a computation requires suspending it via $\mathtt{thunk}\ \mathit{id}$. This
is the only way to turn it into a value type  $\downarrow(A^+ \rightarrow\\,\uparrow\\!{}A^+)$.
--&gt;
&lt;p&gt;Next, let us try a combinator $\mathit{twice}_A$ of type $(A \rightarrow A) \rightarrow A \rightarrow A$.
It takes a function $f$ and and argument $x$ and applies $f$ to $x$ twice.
In our polarized $\lambda$ calculus, we need to use the other shift
operation and end up with: &lt;&#x2F;p&gt;
&lt;!-- a term of type $\downarrow(A^+ \rightarrow\\,\uparrow\\!{}A^+) \rightarrow A^+ \rightarrow\\,\uparrow\\!A^-$: --&gt;
&lt;!--
$$\lambda f:\\~\\~\downarrow(A^+ \rightarrow\\, \uparrow\\!A^+). \lambda x:A^+.\ \ldots(\mathrm{exercise!})\ldots$$
--&gt;
&lt;p&gt;$$
\begin{array}{l}
\lambda f:\,\downarrow(A^+ \rightarrow\, \uparrow\!A^+). \lambda x:A^+. \\
\mathtt{let\ val}\ y\ =\ (\mathtt{force}\ f)\ x\ \mathtt{in} \\ 
\mathtt{return}\ (\mathtt{force}\ f)\ y
\end{array}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Note how the intermediary result has a name $y$. (Exercise: what is the type
of this program? If we used an alternative term which didn&#x27;t use $\mathbf{return}$, what
changes?)&lt;&#x2F;p&gt;
&lt;h1 id=&quot;an-interpreter&quot;&gt;An interpreter&lt;&#x2F;h1&gt;
&lt;!--
The let declaration in rule $(\uparrow_E)$ is not merely a convenience for 
local declarations, it is a load-bearing construct of the calculus. In the CBPV book,
it is written $e_1\ \mathtt{to}\ x.\ e_2$. It is very clear that
$e_1$ has to be evaluated and bound to $x$ before any evaluation work
on $e_2$ can begin.

What is maybe less obvious is that instead of the high-level
substitution operation, we only bind and lookup variables. In
particular, there are never any name clashes.
--&gt;
&lt;p&gt;The logical reading of these typing rules is that we have set up a particular kind
of natural deduction calculus. A term that has a type derivation is a proof, and an
introduction followed by an elimitation is clearly a &amp;quot;detour&amp;quot;.
These detours can be removed and these proof normalization steps
correspond to computation steps.&lt;&#x2F;p&gt;
&lt;!--
$$ \mathtt{let\ val}\ x = \mathtt{return}\ s\ \mathtt{in}\ t \longrightarrow_\beta t[x\leftarrow s]$$
$$ \mathtt{force}\ (\mathtt{thunk}\ t) \longrightarrow_\beta t $$
--&gt;
&lt;p&gt;At this point, we can write out local reductions:&lt;&#x2F;p&gt;
&lt;p&gt;$$\begin{array}{ll}
\mathtt{let\ val}\ x\ =\ \mathtt{return}\ V\ \mathtt{in} \mathtt{M} &amp;amp;\rightsquigarrow M[x := V] \\
\mathtt{thunk}\ (\mathtt{force}\ M) &amp;amp;\rightsquigarrow M
\end{array}
$$&lt;&#x2F;p&gt;
&lt;p&gt;This is not what we were after though. Instead, we can define an &lt;em&gt;abstract machine&lt;&#x2F;em&gt;
that specifies exactly which rewriting steps to take when. What is special about
an abstract machine is that unlike an interpreter that recursively traverses
an expression, the machine always operates at a bounded depth from the top.&lt;&#x2F;p&gt;
&lt;p&gt;What follows are transitions rules of a CK machine. Here C stands for control and K is a stack of
contexts. 
The source level $\mathbf{let\ val}\ x\ = \_ \ \mathbf{in}\ M$ expression is shortened to $(\_ \ \mathtt{to}\ x. M)$,
and an application where we are waiting for the operator to be evaluated is written $(\_\ V)$.
This gives a simple operational semantics, although a CK machine is still a rather high-level description since
we need to appeal to substitution in the definition.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{array}{llll}
C                                        &amp;amp; K &amp;amp; \rightsquigarrow &amp;amp; C&#x27; &amp;amp; K&#x27; \\
\mathtt{let\ val}\ x = M\ \mathtt{in}\ N &amp;amp; k &amp;amp;                 &amp;amp; M  &amp;amp; (\_ \ \mathtt{to}\ x. N) :: k \\
\mathtt{return}\ V                       &amp;amp; (\_\ \mathtt{to}\ x. M) :: k &amp;amp; &amp;amp; M[x := V] &amp;amp;  k  \\
\mathtt{force} (\mathtt{thunk}\ M)       &amp;amp; k   &amp;amp;   &amp;amp; M   &amp;amp;  k \\
M\ V                                     &amp;amp; k   &amp;amp;   &amp;amp; M   &amp;amp; (\_ \ V) :: k \\
\lambda x. M &amp;amp; (\_ \ V) :: k &amp;amp; &amp;amp; M[x := V] &amp;amp; k
\end{array}
$$&lt;&#x2F;p&gt;
&lt;p&gt;There is a simple idea behind all this which is worth restating: we statically (through the type system) know 
that every application $(M\ V)$ comes with an operand that is a value. So:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;whenever we evaluate an application we start by pushing a value (the operand)&lt;&#x2F;li&gt;
&lt;li&gt;when we are done with evaluating the operator and obtain a $\lambda$-term, we can pop a value and continue&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;More precisely, what we push and pop is an application (evaluation context) with
hole in the operator place and an value as operand.
Even though this looks like call-by-value, this subsumes call-by-name because a suspended computation
can be treated as a value. The stack is a list of nested contexts. In a sense,
it is dual to an expression; this can be made precise but we won&#x27;t do this now.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;a-preview-of-the-continuation&quot;&gt;A preview of the continuation&lt;&#x2F;h1&gt;
&lt;p&gt;We started from a natural deduction calculus, which is used for formal logical reasoning, and ended up with an abstract machine.
Unlike rewriting, we have made a step towards a more mechanical, low-level way of normalizing expressions. The fact that the
CK machine is still using substitutions makes it look like we are playing a formal game of symbol manipulation, but if
we could continue from here towards a CEK machine which replaces substitutions with environments.&lt;&#x2F;p&gt;
&lt;p&gt;On the logical side, since we did not discuss products and sums, we are missing conjunction and disjunction. We did not
discuss polarization much. We did not explore classical reasoning, negation, sequent calculus.&lt;&#x2F;p&gt;
&lt;p&gt;We did not talk about effects yet. CBPV gives us a handle on computational effects, similar to monads but different.
It should be obvious how a lean way to specifying evaluation order helps with describing effects.&lt;&#x2F;p&gt;
&lt;p&gt;This is a good time to pause and reflect, before we go to the next round and shed light on some of these topics. In
the meantime, here are some pointers to learn more:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Frank Pfenning&#x27;s 2016 &lt;a href=&quot;https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~fp&#x2F;courses&#x2F;15816-f16&#x2F;schedule.html&quot;&gt;lecture notes on substructual logic&lt;&#x2F;a&gt; has a chapter on CBPV.
It ends not with a CK machine, but a specification of operational semantics in an ordered logic
formalism.&lt;&#x2F;li&gt;
&lt;li&gt;CBPV is described in the book &amp;quot;Call-by-Push-Value: A Functional Imperative Synthesis&amp;quot; by Paul Blain Levy. A minor difference is that
we spell out the application context with the value here while in Levy&#x27;s stack only the value gets pushed.&lt;&#x2F;li&gt;
&lt;li&gt;Matthias Felleisen&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;felleisen.org&#x2F;matthias&#x2F;4400-s20&#x2F;lecture23.html&quot;&gt;lecture notes&lt;&#x2F;a&gt; have a discussion of CK, CEK, CESK machines.
The introduction of environments lets us get rid of substitutions and replace them with environment lookups. This is not yet an
efficient language implementation, but it closes the gap. There is discussion about treating the environment more like a call stack, including
popping unused values. And (for the CESK) there is a discussion on allocating structures.&lt;&#x2F;li&gt;
&lt;li&gt;Bob Harper&#x27;s post on &lt;a href=&quot;https:&#x2F;&#x2F;existentialtype.wordpress.com&#x2F;2012&#x2F;08&#x2F;25&#x2F;polarity-in-type-theory&#x2F;&quot;&gt;polarity in type theory&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Zena M. Ariola, Aaron Bohannon, Amr Sabry. &lt;a href=&quot;https:&#x2F;&#x2F;legacy.cs.indiana.edu&#x2F;~sabry&#x2F;papers&#x2F;sequent.pdf&quot;&gt;Sequent calculi and abstract machines&lt;&#x2F;a&gt; has a
thorough discussion of natural deduction.&lt;&#x2F;li&gt;
&lt;li&gt;Nick Benton, Gavin Bierman, Valeria de Paiva, Martin Hyland. &lt;a href=&quot;https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;2648556_Term_Assignment_for_Intuitionistic_Linear_Logic_Preliminary_Report&quot;&gt;Term Assignment for Intuitionistic Linear Logic&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;!--
Natural deduction as introduced by Gentzen has a normalization theorem: a proof derivation
can be rewritten in a way that it contains no detours. This result is analogous to
cut-elimination result for sequent calculus, but the correspondence is subtle. While
in sequent calculus, we interpret the cut-rule as computation steps, in natural deduction
we have local reductions.

We could also have predefined constants like $\mathtt{plus}: Int^+ \rightarrow Int^+ \rightarrow Int^-$.

Since we are talking about programs, we want to compose computations, that is, build larger computations out of smaller ones.
--&gt;
</content>
    </entry>
</feed>
